{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lat Long Analysis\n",
    "\n",
    "This portion of our project uses the schools' latitude and longitude (previously obtained using geocoder) to perform some final cleaning on the VADIR data and then join it to the NYC crime data by location. The code below will:  \n",
    "\n",
    "* __Load vadir data__ and cleaning it with the functions from cleandata.py  \n",
    "* Ensure __schools are consistently named__ (we'll use the names from the lat long file and join them using the beds/seds code. This also means that we'll discard records from the schools for which we don't have lat/long)  \n",
    "* __Fill in missing boroughs__ for records from 2006-2007.  \n",
    "* __Identify felonies within a 1 mile__ radius of a given school.  \n",
    "* __Plot correlations__ between school indicents and felonies (by year, by borough, by felony type, by location, by school incident type)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "% matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from vincenty import vincenty\n",
    "import cleandata as cd\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in and clean the VADIR data from the 2006-2014 school years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... data from VADIR_2006.xls appended. Added 1455 rows for a total of 1455.\n",
      "... data from VADIR_2007.xls appended. Added 1500 rows for a total of 2955.\n",
      "... data from VADIR_2008.xls appended. Added 1545 rows for a total of 4500.\n",
      "... data from VADIR_2009.xls appended. Added 1531 rows for a total of 6031.\n",
      "... data from VADIR_2010.xls appended. Added 1678 rows for a total of 7709.\n",
      "... data from VADIR_2011.xls appended. Added 1693 rows for a total of 9402.\n",
      "... data from VADIR_2012.xls appended. Added 1735 rows for a total of 11137.\n",
      "... data from VADIR_2013.xls appended. Added 1792 rows for a total of 12929.\n",
      "... data from VADIR_2014.xls appended. Added 1805 rows for a total of 14734.\n"
     ]
    }
   ],
   "source": [
    "# Function to load and clean VADIR data\n",
    "def load_and_clean_VADIR():\n",
    "    \"\"\"\n",
    "    Function to load, join and clean VADIR (performs a series\n",
    "    of functions from the cleandata module). Returns school\n",
    "    dataframe ready for analysis.\n",
    "    \"\"\"\n",
    "    # Raw data for each year\n",
    "    RAW_DATA_DICT = {2006: 'VADIR_2006.xls', 2007: 'VADIR_2007.xls', 2008: 'VADIR_2008.xls', \n",
    "                     2009: 'VADIR_2009.xls', 2010: 'VADIR_2010.xls', 2011: 'VADIR_2011.xls', \n",
    "                     2012: 'VADIR_2012.xls', 2013: 'VADIR_2013.xls', 2014: 'VADIR_2014.xls'}\n",
    "\n",
    "    # Duplicate name columns in raw files (and their replacements)\n",
    "    DUP_COLS = {'County Name':'County', 'District Name': 'District', 'BEDS CODE': 'BEDS Code', \n",
    "                'False Alarm':'Bomb Threat False Alarm',\n",
    "                'Other Sex offenses': 'Other Sex Offenses', \n",
    "                'Use Possession or Sale of Drugs': 'Drug Possession', \n",
    "                'Use Possession or Sale of Alcohol': 'Alcohol Possession',\n",
    "                'Other Disruptive Incidents': 'Other Disruptive Incidents', \n",
    "                'Drug Possesion': 'Drug Possession', 'Alcohol Possesion': 'Alcohol Possession', \n",
    "                'Other Disruptive': 'Other Disruptive Incidents'}\n",
    "\n",
    "    # Read in raw data and correct duplicate columns\n",
    "    vadir_df = cd.vadir_concat_dfs(RAW_DATA_DICT, DUP_COLS)\n",
    "    \n",
    "    # Reorder columns putting demographic information first.\n",
    "    DEMO_COLS = ['School Name', 'School Type', 'School Year', 'BEDS Code',  'County', \n",
    "                 'District', 'Enrollment', 'Grade Organization', 'Need/Resource Category']\n",
    "    vadir_df = cd.vadir_reorder_columns(vadir_df, DEMO_COLS)\n",
    "    \n",
    "    # Create Columns for \"Total incidents\", \"Incidents w/out Weapons\" and \"Incidents w/ Weapons\"\n",
    "    COLUMNS = vadir_df.columns.tolist()\n",
    "    INCIDENT_COLS = [c for c in COLUMNS if c not in DEMO_COLS]\n",
    "    vadir_df = cd.vadir_create_tallies(vadir_df, INCIDENT_COLS)\n",
    "    \n",
    "    # Consistently name county and school type, fix name capitalization, remove comment rows.\n",
    "    school_df = cd.vadir_clean_concat_df(vadir_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function call\n",
    "school_df = load_and_clean_VADIR()\n",
    "\n",
    "# Take a look -- uncomment to run\n",
    "#school_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Join School Data and Location Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read in lat-long file that contains correct names and locations\n",
    "#... ??? replace this call with function call to Aaron's geocoder function???\n",
    "latlon_df = pd.read_csv('SchoolLatLon.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helper function -- extract lat/long from object type\n",
    "def parse_latlong(dataframe, loc_column):\n",
    "    \"\"\"\n",
    "    Function to extract lat/long coords. \n",
    "    INPUT: dataframe and name of column with string tuple or list pair of coordinates.\n",
    "    OUTPUT: n/a. Function modifies dataframe to add a lat and long column with float type.\n",
    "    \"\"\"\n",
    "    get_lat = lambda x: x.split(',')[0][1:] if type(x)==type('s') else np.nan\n",
    "    get_long = lambda x: x.split(',')[1][:-1] if type(x)==type('s') else np.nan\n",
    "    dataframe['lat'] = dataframe[loc_column].apply(get_lat).astype('float64')\n",
    "    dataframe['long'] = dataframe[loc_column].apply(get_long).astype('float64')\n",
    "    print('... latitude and longitude extracted for dataframe.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Helper function to add latitude and longitudes to school data frame\n",
    "def join_latlong(school_df, latlon_df, talk=False):\n",
    "    \"\"\"\n",
    "    Function to add (and parse)latitude and longitude \n",
    "    information for each school.\n",
    "    INPUT: school dataframe including \"BEDS Code\" column,\n",
    "           latitude/longitude dataframe w/ \"SED CODE\" col.\n",
    "           (optional 'talk' bool turns on/off print statements)\n",
    "    OUTPUT: dataframe with school data plut new columns \n",
    "            for 'LEGAL NAME', 'Full_Address', 'latlon'(obj),\n",
    "            lat(float64) and long(float64).\n",
    "    \"\"\"\n",
    "    # ensure BEDS and SED are integers so that they'll be recognized as identical\n",
    "    latlon_df[\"SED CODE\"] = latlon_df[\"SED CODE\"].astype(np.int64)\n",
    "    school_df[\"BEDS Code\"] = school_df[\"BEDS Code\"].astype(np.int64)\n",
    "    \n",
    "    # join latlong data to school data using the BEDS code\n",
    "    school_df = pd.merge(school_df, latlon_df, left_on=['BEDS Code'],right_on=['SED CODE'], how='left')\n",
    "    \n",
    "    # parse latlon object in to numerical columns\n",
    "    parse_latlong(school_df, 'latlon')\n",
    "    \n",
    "    # drop the now redundant SED code\n",
    "    school_df.drop(['SED CODE'], axis=1, inplace=True)\n",
    "    \n",
    "    # Take a look at the resulting data/missing values\n",
    "    if talk:\n",
    "        print('... joined df inclues {} unique schools,'.format(len(school_df['BEDS Code'].unique())))\n",
    "        schools_withloc = school_df[school_df['latlon'].notnull()]['BEDS Code'].unique()\n",
    "        schools_missingloc = school_df[school_df['latlon'].isnull()]['BEDS Code'].unique()\n",
    "        print('... of which {} have lat/long'.format(len(schools_withloc)),\n",
    "              'and {} are missing lat/long'.format(len(schools_missingloc)))\n",
    "        \n",
    "    return school_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... 1967 unique schools,\n",
      "... of which 1807 have lat/long\n",
      "... and 160 are missing lat/long\n"
     ]
    }
   ],
   "source": [
    "# Function Call\n",
    "school_df = join_latlong(school_df, latlon_df, talk=True)\n",
    "\n",
    "# Take a look -- uncomment to run\n",
    "#school_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fix School Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helper Function\n",
    "def fix_case(x):\n",
    "    \"\"\"Function to put a school name in the correct case\"\"\"\n",
    "    if not x:\n",
    "        return x\n",
    "    elif x[:3] in ['PS ', 'JHS', 'MS ']:\n",
    "        return x[:3] + x[3:].title()\n",
    "    else:\n",
    "        return x.title()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... original dataset has 3135 unique school names\n",
      "... but only 1967 unique BEDS Codes\n"
     ]
    }
   ],
   "source": [
    "# Helper function to eliminate duplicated school names\n",
    "def fix_school_names(school_df, talk=False):\n",
    "    \"\"\" \n",
    "    INPUT: school_df with original 'School Name' column\n",
    "           and with 'LEGAL NAME' column from latlong file.\n",
    "           (optional 'talk' bool turns on/off print statements)\n",
    "    OUTPUT: n/a, fxn modifies school_df to replace duplicate\n",
    "           'School Name's with their (consistent) legal name.\n",
    "    \"\"\"\n",
    "    if talk:\n",
    "        print('... original dataset had {} unique'.format(len(school_df['School Name'].unique())),\n",
    "              'school names but only {} unique BEDS Codes'.format(len(school_df['BEDS Code'].unique())))\n",
    "    \n",
    "    # Fix missing LEGAL NAMES with School Name\n",
    "    school_df['LEGAL NAME'].fillna(school_df['School Name'], inplace=True)\n",
    "    # Fix case and reassign to School Name\n",
    "    school_df['School Name'] = school_df['LEGAL NAME'].apply(fix_case)\n",
    "    # drop the now redundant LEGAL NAME column\n",
    "    school_df.drop(['LEGAL NAME'], axis=1, inplace=True)\n",
    "    \n",
    "    if talk:\n",
    "        print('... new dataset has {} unique school '.format(len(school_df['School Name'].unique())),\n",
    "              'names and {} unique BEDS Codes.'.format(len(school_df['BEDS Code'].unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# function call\n",
    "fix_school_names(school_df)\n",
    "\n",
    "# Take a look -- uncomment to run\n",
    "#school_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill in missing boroughs\n",
    "\n",
    "__TODO:__ From the numbers below it looks like some of the County values got switched around (eg. decrease in Manhattan counts?)... I think we probalby need a way of creating the county_map dictionary that prioritizes the traditional borough name). Come back to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helper function to fill in boroughs\n",
    "def fill_in_boroughs(school_df, talk=False):\n",
    "    \"\"\" \n",
    "    INPUT: school_df with 'County' and 'BEDS Code' columns\n",
    "           (optional 'talk' bool turns on/off print statements)\n",
    "    OUTPUT: n/a, fxn modifies school_df to fill in boroughs.\n",
    "    \"\"\"\n",
    "    if talk:\n",
    "        print('... Originally, {} entries were missing'.format(sum(school_df['County'].isnull())),\n",
    "              ' county info. Other counties:\\n',school_df.County.value_counts())\n",
    "\n",
    "    # create dictionary of county by BEDS Code\n",
    "    c = school_df[school_df['County'].notnull()][['BEDS Code','County']].to_dict()\n",
    "    county_map = {c['BEDS Code'][idx]: c['County'][idx] for idx in c['County'].keys()}\n",
    "    # map counties using dictionary\n",
    "    school_df.County = school_df['BEDS Code'].map(county_map)\n",
    "    \n",
    "    if talk:\n",
    "        print('... Now {} entries are missing'.format(sum(school_df['County'].isnull())),\n",
    "              ' county info. Other counties:\\n',school_df.County.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function call\n",
    "fill_in_boroughs(school_df, talk=True)\n",
    "\n",
    "# Take a look -- uncomment to run\n",
    "#school_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    1967\n",
       "Name: County, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# QUICK CHECK - are any BEDS Codes are linked with more than one Borough(County)?\n",
    "school_df.groupby('BEDS Code')['County'].apply(lambda x: len(x.unique())).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prep for distance sorting crime locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helper function -- check dist\n",
    "def is_in_radius(school_point, crime_point, radius):\n",
    "    \"\"\"\n",
    "    Function using vincenty package to check distance between school and crime.\n",
    "    INPUT: (lat,long) tuples for school and crime (in degrees), radius in miles.\n",
    "    OUTPUT: Boolean\n",
    "    \"\"\"\n",
    "    return vincenty(school_point, crime_point, miles=True) <= radius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load NYC dataframe\n",
    "felony_df = pd.read_csv('NYPD_7_Major_Felony_Incidents.csv', index_col = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ... and clean it   \n",
    "#felony_df = cd.clean_NYPD(felony_df)\n",
    "#(I'm getting and ERROR here... need to return to NYPD cleaning function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... latitude and longitude extracted for dataframe.\n",
      "... latitude and longitude extracted for dataframe.\n"
     ]
    }
   ],
   "source": [
    "# Extact Lattitude and longitude data for felony dataframes\n",
    "parse_latlong(school_df, 'latlon')\n",
    "parse_latlong(felony_df, 'Location 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance:  13.120085\n",
      "... w/in 2 mi? False\n",
      "... w/in 50 mi? True\n"
     ]
    }
   ],
   "source": [
    "# Testing vincenty on the first felony and first school\n",
    "first_school_point = (school_df.loc[0,'lat'], school_df.loc[0,'long']) \n",
    "first_felony_point = (felony_df.loc[1,'lat'], felony_df.loc[1,'long']) \n",
    "\n",
    "# not w/in 2 miles, but yes, w/in 50\n",
    "print('Distance: ', vincenty(first_school_point, first_felony_point))\n",
    "print(\"... w/in 2 mi?\", is_in_radius(first_school_point, first_felony_point, 2))\n",
    "print(\"... w/in 50 mi?\",is_in_radius(first_school_point, first_felony_point, 50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to extract crime tallies w/in radius of schools\n",
    "\n",
    "I suspect that looping through each school is going to take forever... but we'll try that first and then explore a smarter (dynamic programming) alternative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... there are 640 missing latlon entries\n",
      "... there are 640 missing lat entries\n"
     ]
    }
   ],
   "source": [
    "# Quick Check, are there rows with 'latlon' but not 'lat'\n",
    "print('... there are {} missing latlon entries'.format(sum(school_df.latlon.isnull())))\n",
    "print('... there are {} missing lat entries'.format(sum(school_df.lat.isnull())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 1: Helper functions for setting up a grid for NYC lat/long coords\n",
    "NOTES: The max lat of a school is ~ 40.9  and the distance between 40.9 and 40.95 is over 3 miles... but there are 7 crimes that fell under the jurisdiction of the NY Transit police whose locations are recorded north of 41 degrees (the farthes one is 500 miles away). The minimum longitude of a school is ~-74.24 which is around 3 miles from -74.3. There are 63 crimes that occurred west of -74.3. I suggest that we disregard these outliers for the purposes of our analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latitude ranges from 40.5078027 to 40.9034547 with a total distance of 27.300837\n",
      "Longitude ranges from -74.2440251 to -73.4883134 with a total distance of 52.408039\n"
     ]
    }
   ],
   "source": [
    "# Initial exploration of ranges\n",
    "max_lat = school_df.lat.max()\n",
    "min_lat = school_df.lat.min()\n",
    "max_long = school_df.long.max()\n",
    "min_long = school_df.long.min()\n",
    "\n",
    "lat_dist = vincenty((min_lat, 0.5*(max_long + min_long)),(max_lat, 0.5*(max_long + min_long)), miles=True)\n",
    "long_dist = vincenty((min_long, 0.5*(max_lat + min_lat)),(max_long, 0.5*(max_lat + min_lat)), miles=True)\n",
    "\n",
    "print('Latitude ranges from {} to {} with a total distance of {}'.format(min_lat, max_lat, lat_dist))\n",
    "print('Longitude ranges from {} to {} with a total distance of {}'.format(min_long, max_long, long_dist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Helper function to identify grid cell that contains a given point\n",
    "def nyc_grid(lat,long):\n",
    "    \"\"\"\n",
    "    This function identifies a square mile cell of NYC that contains \n",
    "    the given longitude and latitude point. There are 1500 cells in \n",
    "    total. 30 rows each represent a segement of latitude and 50 \n",
    "    columns each represent a segment of longitude. The cells are \n",
    "    numbered 0 through 1599 and they are unique to this analysis.\n",
    "    \"\"\"\n",
    "    # max and min values from data set\n",
    "    max_lat = 40.95\n",
    "    min_lat = 40.50\n",
    "    max_long = -73.45\n",
    "    min_long = -74.30\n",
    "    \n",
    "    # divide each range into segments of a little over a mile\n",
    "    delta_lat = (max_lat - min_lat)/28\n",
    "    delta_long = (max_long - min_long)/48\n",
    "\n",
    "    # then segment each direction\n",
    "    lat_seg = np.array([min_lat + idx*delta_lat for idx in range(-1,29)])\n",
    "    long_seg = np.array([min_long + idx*delta_long for idx in range(-1,49)])\n",
    "\n",
    "    # identify where given point fits in segments\n",
    "    row = sum(lat_seg <= lat) - 1\n",
    "    col = sum(long_seg <= long) - 1\n",
    "    \n",
    "    # return grid number\n",
    "    if row < 0 or row == 29 or col < 0 or col == 49:\n",
    "        return np.nan\n",
    "    else:\n",
    "        return row * 50 + col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test an out of bound point\n",
    "nyc_grid(40.653161, -76.862164)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1074"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test an in bound point\n",
    "nyc_grid(40.821798, -73.886463)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# helper function to get a list of adjacent cells\n",
    "def get_adjacent(cell_num):\n",
    "    \"\"\" \n",
    "    This function identifies a group of cells which together superset \n",
    "    any points within a mile of any location in the original cell.\n",
    "    INPUT: a cell number (< 5999) from NYC grid\n",
    "    OUTPUT: a list of adjacent and or diagonal cell numbers\n",
    "    \n",
    "    NOTE: this function should only be run on cell numbers of vadir\n",
    "    school locations since the nyc_grid is designed so that all\n",
    "    schools are in a cell that is not a boarder cell.\n",
    "    \"\"\"\n",
    "    col = cell_num % 50\n",
    "    row = cell_num // 50\n",
    "    row_range = [row - 1, row, row + 1]\n",
    "    col_range = [col - 1, col, col + 1]\n",
    "    return [r * 50 + c for r in row_range for c in col_range]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 51, 52, 53, 101, 102, 103]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test grid adjacency\n",
    "get_adjacent(52)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# helper function to get subset of felonies within a 1 mile radius\n",
    "def get_local_crimes(location, felony_df):\n",
    "    \"\"\"\n",
    "    This function identifies crimes less than 1 mi \n",
    "    from a single school (or GPS location).\n",
    "    INPUT: location (GPS point), felony_df (w/\n",
    "        'NYC_grid column already populated)\n",
    "    OUTPUT: felony_df subset of crimes that \n",
    "        occurred within one mile of school.\n",
    "            \n",
    "    \"\"\"\n",
    "    cells_to_search = get_adjacent(nyc_grid(*location))\n",
    "    print('   LOCATION: {}, \\n   GRID CELLS: {}'.format(location, cells_to_search)) # for debugging\n",
    "    \n",
    "    # Get subset of crimes w/in grid\n",
    "    tick1 = time.clock() # for debugging\n",
    "    cells_to_search = get_adjacent(nyc_grid(*location)) \n",
    "    crimes = felony_df.loc[felony_df.NYC_grid.isin(cells_to_search)]\n",
    "    tock1 = time.clock() # for debugging\n",
    "    print('    ... found {} crimes w/in 9 cells [{} seconds]'.format(len(crimes),tock1 - tick1)) # for debugging\n",
    "    \n",
    "    # Further subset by a radius of 1 mile\n",
    "    if not crimes.empty: \n",
    "        tick2 = time.clock() # for debugging\n",
    "        r_filter = lambda x: is_in_radius(location,(x.lat,x.long),1)\n",
    "        crimes =  crimes[crimes.apply(r_filter, axis=1)]  \n",
    "        tock2 = time.clock() # for debugging \n",
    "        print('    ... of them {} are w/in one mile [{} seconds]'.format(len(crimes), tock2 - tick2)) # for debugging\n",
    "    return crimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   LOCATION: (40.8213673, -73.4883134), \n",
      "   GRID CELLS: [995, 996, 997, 1045, 1046, 1047, 1095, 1096, 1097]\n",
      "    ... found 0 crimes w/in 9 cells [0.13741499999997586 seconds]\n"
     ]
    }
   ],
   "source": [
    "# test function on a single school / no crimes\n",
    "this_school = school_df[school_df['BEDS Code'] == 307500014256]\n",
    "location = (this_school.lat.mean(), this_school.long.mean())\n",
    "result = get_local_crimes(location, felony_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   LOCATION: (40.81890629999999, -73.9053333), \n",
      "   GRID CELLS: [972, 973, 974, 1022, 1023, 1024, 1072, 1073, 1074]\n",
      "    ... found 61870 crimes w/in 9 cells [0.255766000000051 seconds]\n",
      "    ... of them 31860 are w/in one mile [2.8463949999999727 seconds]\n"
     ]
    }
   ],
   "source": [
    "# now testing on a school with crimes\n",
    "this_school = school_df[school_df['BEDS Code'] == 307500012017]\n",
    "location = (this_school.lat.mean(), this_school.long.mean())\n",
    "result = get_local_crimes(location, felony_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 2: Crime counting function to search only within adjacent cells of the school\n",
    "\n",
    "NOTE: loading grid cell#s for the felony data set takes 3-4 minutes and only needs to be done once. To skip that step after you've already run this function before simply set the optional parameter skip_gridsetup to True (it defaults False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tally_crime_by_loc(school_df, felony_df, skip_gridsetup = False):\n",
    "    \"\"\"\n",
    "    Function to tally and store information about felonies \n",
    "    that occur w/in one mile of each school in the school_df.\n",
    "    INPUT: school df w/ cols 'latlon', 'lat', 'long', and 'School Year'\n",
    "           felony df w/ cols 'Occurrence Year', 'lat','long','Offense', and 'Identifier'\n",
    "    OUTPUT: n/a, modifies school data.\n",
    "    \"\"\"\n",
    "    tick = time.clock() # for debugging\n",
    "    \n",
    "    if not skip_gridsetup:\n",
    "        # prepare felony dataframe by adding a column for nyc_grid cell number\n",
    "        tick0 = time.clock() # for debugging\n",
    "        felony_df.lat.fillna(0, inplace=True)\n",
    "        felony_df.long.fillna(0, inplace=True)\n",
    "        felony_df['NYC_grid'] = felony_df.apply(lambda x: nyc_grid(x.lat, x.long),axis=1)\n",
    "        tock0 = time.clock() # for debugging\n",
    "        print('... calculated felony grid cells [{} seconds]'.format(tock0 - tick0)) # for debugging\n",
    "    \n",
    "    # Initialize new columns in school data frame\n",
    "    school_df['CrimeIDS'] = pd.Series()\n",
    "    school_df['Total Felonies w/in 1mi'] = pd.Series()\n",
    "    school_df['Grand Larceny w/in 1mi'] = pd.Series()\n",
    "    school_df['Robbery w/in 1mi'] = pd.Series()\n",
    "    school_df['Burglary w/in 1mi'] = pd.Series()\n",
    "    school_df['Assault w/in 1mi'] = pd.Series()\n",
    "    school_df['Auto Theft w/in 1mi'] = pd.Series()\n",
    "    school_df['Rape w/in 1mi'] = pd.Series()\n",
    "    school_df['Murders w/in 1mi'] = pd.Series()\n",
    "    \n",
    "    # Group schools (unique location for each BEDS Code) \n",
    "    grouped = school_df[school_df.lat.notnull()].groupby(['BEDS Code'])\n",
    "    print('... found {} unique schools'.format(len(grouped.groups))) # for debugging\n",
    "    \n",
    "    # Loop through schools \n",
    "    for beds, df in grouped:\n",
    "        tick0 = time.clock() # for debugging\n",
    "        print('>>> now processing:', beds) # for debugging\n",
    "        # NOTE: the coordinates should all be the same so the mean is just the location\n",
    "        assert len(df.lat.unique().tolist()) == 1, 'ERROR: multiple latitudes for this school.'\n",
    "        location = (df.lat.mean(), df.long.mean())\n",
    "        local_crimes = get_local_crimes(location, felony_df)\n",
    "\n",
    "        # tally and store felonies for each year\n",
    "        tick3 = time.clock()  # for debugging\n",
    "        for year in df['School Year'].unique():\n",
    "            subset = local_crimes[local_crimes['Occurrence Year'] == year]\n",
    "            idxs = df[df['School Year'] == year].index.tolist()\n",
    "            school_df.loc[idxs,['CrimeIDS']] = str(subset.Identifier.unique().tolist())\n",
    "            school_df.loc[idxs,['Total Felonies w/in 1mi']] = len(subset)        \n",
    "            school_df.loc[idxs,['Grand Larceny w/in 1mi']] = sum(subset['Offense'] == 'GRAND LARCENY')\n",
    "            school_df.loc[idxs,['Robbery w/in 1mi']] = sum(subset['Offense'] == 'ROBBERY')\n",
    "            school_df.loc[idxs,['Burglary w/in 1mi']] = sum(subset['Offense'] == 'BURGLARY')\n",
    "            school_df.loc[idxs,['Assault w/in 1mi']] = sum(subset['Offense'] == 'FELONY ASSAULT')\n",
    "            school_df.loc[idxs,['Auto Theft w/in 1mi']] = sum(subset['Offense'] == 'GRAND LARCENY OF MOTOR VEHICLE')\n",
    "            school_df.loc[idxs,['Rape w/in 1mi']] = sum(subset['Offense'] == 'RAPE')\n",
    "            school_df.loc[idxs,['Murders w/in 1mi']] = sum(subset['Offense'] == 'MURDER & NON-NEGL. MANSLAUGHTE')\n",
    "        tock3 = time.clock() \n",
    "        print('    ... tallied year counts [{} seconds]'.format(tock3 - tick3)) # for debugging        \n",
    "        tock0 = time.clock() # for debugging  \n",
    "        print('   TIME: {} seconds.'.format( beds, tock0 - tick0)) # for debugging\n",
    "        \n",
    "    tock = time.clock()  # for debugging  \n",
    "    print('\\n IN TOTAL : {} seconds'.format(tock - tick)) # for debugging  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<      Calculate felony grids: 194.05195899999998 seconds>\n",
      "... found 1807 unique schools\n",
      "     < Find crimes in grid: 0.38215500000001157 seconds>\n",
      "     < Find crimes w.in 1mi: 3.4214620000000195 seconds>\n",
      "     < Add year counts: 0.09680199999999672 seconds>:\n",
      ">>> Processed 300000010777 in 3.901927999999998 seconds.\n",
      "     < Find crimes in grid: 0.31295800000000895 seconds>\n",
      "     < Find crimes w.in 1mi: 4.471405000000004 seconds>\n",
      "     < Add year counts: 0.679380000000009 seconds>:\n",
      ">>> Processed 307500011035 in 5.465260000000001 seconds.\n",
      "     < Find crimes in grid: 0.26682699999997794 seconds>\n",
      "     < Find crimes w.in 1mi: 3.1204560000000185 seconds>\n",
      "     < Add year counts: 0.4236370000000136 seconds>:\n",
      ">>> Processed 307500011079 in 3.812283000000008 seconds.\n",
      "     < Find crimes in grid: 0.18426900000000046 seconds>\n",
      "     < Find crimes w.in 1mi: 0.7779110000000173 seconds>\n",
      "     < Add year counts: 0.17704899999998247 seconds>:\n",
      ">>> Processed 307500011094 in 1.1407230000000084 seconds.\n",
      "     < Find crimes in grid: 0.2828299999999899 seconds>\n",
      "     < Find crimes w.in 1mi: 3.7202330000000075 seconds>\n",
      "     < Add year counts: 0.4267669999999839 seconds>:\n",
      ">>> Processed 307500011138 in 4.431534999999997 seconds.\n",
      "     < Find crimes in grid: 0.2776980000000151 seconds>\n",
      "     < Find crimes w.in 1mi: 3.69764600000002 seconds>\n",
      "     < Add year counts: 0.2856130000000121 seconds>:\n",
      ">>> Processed 307500011169 in 4.262340999999992 seconds.\n",
      "     < Find crimes in grid: 0.3553189999999802 seconds>\n",
      "     < Find crimes w.in 1mi: 5.812004000000002 seconds>\n",
      "     < Add year counts: 0.6041670000000181 seconds>:\n",
      ">>> Processed 307500011226 in 6.772838000000007 seconds.\n",
      "     < Find crimes in grid: 0.17399199999999837 seconds>\n",
      "     < Find crimes w.in 1mi: 0.7972769999999798 seconds>\n",
      "     < Add year counts: 0.10993700000000217 seconds>:\n",
      ">>> Processed 307500011401 in 1.0829029999999875 seconds.\n",
      "     < Find crimes in grid: 0.31884399999998436 seconds>\n",
      "     < Find crimes w.in 1mi: 4.958350999999993 seconds>\n",
      "     < Add year counts: 0.5185290000000009 seconds>:\n",
      ">>> Processed 307500011721 in 5.797423000000009 seconds.\n",
      "     < Find crimes in grid: 0.3543280000000095 seconds>\n",
      "     < Find crimes w.in 1mi: 5.804393000000005 seconds>\n",
      "     < Add year counts: 0.6534680000000037 seconds>:\n",
      ">>> Processed 307500011751 in 6.813603999999998 seconds.\n",
      "     < Find crimes in grid: 0.27966200000000185 seconds>\n",
      "     < Find crimes w.in 1mi: 3.6306940000000054 seconds>\n",
      "     < Add year counts: 0.24818999999999392 seconds>:\n",
      ">>> Processed 307500011811 in 4.16026500000001 seconds.\n",
      "     < Find crimes in grid: 0.1702280000000087 seconds>\n",
      "     < Find crimes w.in 1mi: 0.7904860000000156 seconds>\n",
      "     < Add year counts: 0.13355699999999615 seconds>:\n",
      ">>> Processed 307500012010 in 1.0957340000000215 seconds.\n",
      "     < Find crimes in grid: 0.20030300000001944 seconds>\n",
      "     < Find crimes w.in 1mi: 1.7434269999999685 seconds>\n",
      "     < Add year counts: 0.20560100000000148 seconds>:\n",
      ">>> Processed 307500012012 in 2.150683000000015 seconds.\n",
      "     < Find crimes in grid: 0.2444110000000137 seconds>\n",
      "     < Find crimes w.in 1mi: 2.7249800000000164 seconds>\n",
      "     < Add year counts: 0.41963299999997616 seconds>:\n",
      ">>> Processed 307500012017 in 3.391012000000046 seconds.\n",
      "     < Find crimes in grid: 0.2980620000000158 seconds>\n",
      "     < Find crimes w.in 1mi: 3.5205869999999777 seconds>\n",
      "     < Add year counts: 0.3694929999999772 seconds>:\n",
      ">>> Processed 307500012168 in 4.189619999999991 seconds.\n",
      "     < Find crimes in grid: 0.1764919999999961 seconds>\n",
      "     < Find crimes w.in 1mi: 0.8362360000000422 seconds>\n",
      "     < Add year counts: 0.17697700000002214 seconds>:\n",
      ">>> Processed 307500012176 in 1.1911850000000186 seconds.\n",
      "     < Find crimes in grid: 0.28849400000001424 seconds>\n",
      "     < Find crimes w.in 1mi: 3.876260000000002 seconds>\n",
      "     < Add year counts: 0.3901369999999815 seconds>:\n",
      ">>> Processed 307500012186 in 4.556339999999977 seconds.\n",
      "     < Find crimes in grid: 0.28528599999998505 seconds>\n",
      "     < Find crimes w.in 1mi: 3.7736679999999865 seconds>\n",
      "     < Add year counts: 0.3397749999999746 seconds>:\n",
      ">>> Processed 307500012188 in 4.4001349999999775 seconds.\n",
      "     < Find crimes in grid: 0.29658799999998564 seconds>\n",
      "     < Find crimes w.in 1mi: 3.891836000000012 seconds>\n",
      "     < Add year counts: 0.3841770000000224 seconds>:\n",
      ">>> Processed 307500012352 in 4.5741199999999935 seconds.\n",
      "     < Find crimes in grid: 0.19324000000000296 seconds>\n",
      "     < Find crimes w.in 1mi: 0.8901470000000131 seconds>\n",
      "     < Add year counts: 0.18670499999996082 seconds>:\n",
      ">>> Processed 307500012721 in 1.2715709999999945 seconds.\n",
      "     < Find crimes in grid: 0.173637000000042 seconds>\n",
      "     < Find crimes w.in 1mi: 0.8373780000000011 seconds>\n",
      "     < Add year counts: 0.16314699999998084 seconds>:\n",
      ">>> Processed 307500012723 in 1.1755540000000337 seconds.\n",
      "     < Find crimes in grid: 0.2450279999999907 seconds>\n",
      "     < Find crimes w.in 1mi: 2.7171659999999633 seconds>\n",
      "     < Add year counts: 0.315433999999982 seconds>:\n",
      ">>> Processed 307500012754 in 3.2790109999999686 seconds.\n",
      "     < Find crimes in grid: 0.25731300000001056 seconds>\n",
      "     < Find crimes w.in 1mi: 3.006702999999959 seconds>\n",
      "     < Add year counts: 0.36227599999995164 seconds>:\n",
      ">>> Processed 307500012811 in 3.6277939999999944 seconds.\n",
      "     < Find crimes in grid: 0.22674100000000408 seconds>\n",
      "     < Find crimes w.in 1mi: 1.8844389999999862 seconds>\n",
      "     < Add year counts: 0.256445000000042 seconds>:\n",
      ">>> Processed 307500013004 in 2.369015999999988 seconds.\n",
      "     < Find crimes in grid: 0.2177319999999554 seconds>\n",
      "     < Find crimes w.in 1mi: 1.881494000000032 seconds>\n",
      "     < Add year counts: 0.29577900000003865 seconds>:\n",
      ">>> Processed 307500013036 in 2.3964090000000056 seconds.\n",
      "     < Find crimes in grid: 0.20502599999997528 seconds>\n",
      "     < Find crimes w.in 1mi: 1.5642550000000028 seconds>\n",
      "     < Add year counts: 0.17291199999999662 seconds>:\n",
      ">>> Processed 307500013053 in 1.9436119999999732 seconds.\n",
      "     < Find crimes in grid: 0.19012800000001562 seconds>\n",
      "     < Find crimes w.in 1mi: 0.7916859999999701 seconds>\n",
      "     < Add year counts: 0.1566310000000044 seconds>:\n",
      ">>> Processed 307500013077 in 1.139916000000028 seconds.\n",
      "     < Find crimes in grid: 0.29639200000002575 seconds>\n",
      "     < Find crimes w.in 1mi: 4.326475000000016 seconds>\n",
      "     < Add year counts: 0.4290959999999586 seconds>:\n",
      ">>> Processed 307500013140 in 5.053384999999992 seconds.\n",
      "     < Find crimes in grid: 0.25208300000002737 seconds>\n",
      "     < Find crimes w.in 1mi: 2.9149620000000027 seconds>\n",
      "     < Add year counts: 0.367354999999975 seconds>:\n",
      ">>> Processed 307500013141 in 3.5357759999999985 seconds.\n",
      "     < Find crimes in grid: 0.20041399999996656 seconds>\n",
      "     < Find crimes w.in 1mi: 1.5863740000000348 seconds>\n",
      "     < Add year counts: 0.19496400000002723 seconds>:\n",
      ">>> Processed 307500013231 in 1.9831740000000195 seconds.\n",
      "     < Find crimes in grid: 0.26132100000000946 seconds>\n",
      "     < Find crimes w.in 1mi: 3.198680999999965 seconds>\n",
      "     < Add year counts: 0.404404999999997 seconds>:\n",
      ">>> Processed 307500013368 in 3.866201999999987 seconds.\n",
      "     < Find crimes in grid: 0.22603099999997767 seconds>\n",
      "     < Find crimes w.in 1mi: 1.8965630000000147 seconds>\n",
      "     < Add year counts: 0.343407999999954 seconds>:\n",
      ">>> Processed 307500013369 in 2.467554000000007 seconds.\n",
      "     < Find crimes in grid: 0.17511299999995344 seconds>\n",
      "     < Find crimes w.in 1mi: 0.8239599999999996 seconds>\n",
      "     < Add year counts: 0.19517600000000357 seconds>:\n",
      ">>> Processed 307500013370 in 1.1959040000000414 seconds.\n",
      "     < Find crimes in grid: 0.17915399999998272 seconds>\n",
      "     < Find crimes w.in 1mi: 1.1571969999999965 seconds>\n",
      "     < Add year counts: 0.17459300000001576 seconds>:\n",
      ">>> Processed 307500013371 in 1.5124349999999822 seconds.\n",
      "     < Find crimes in grid: 0.21360299999997778 seconds>\n",
      "     < Find crimes w.in 1mi: 1.7527630000000158 seconds>\n",
      "     < Add year counts: 0.3028520000000299 seconds>:\n",
      ">>> Processed 307500013372 in 2.270665000000008 seconds.\n",
      "     < Find crimes in grid: 0.26639299999999366 seconds>\n",
      "     < Find crimes w.in 1mi: 3.472586999999976 seconds>\n",
      "     < Add year counts: 0.40556900000001406 seconds>:\n",
      ">>> Processed 307500013373 in 4.145938000000001 seconds.\n",
      "     < Find crimes in grid: 0.2734439999999836 seconds>\n",
      "     < Find crimes w.in 1mi: 3.5014769999999658 seconds>\n",
      "     < Add year counts: 0.42203799999998637 seconds>:\n",
      ">>> Processed 307500013396 in 4.198315999999977 seconds.\n",
      "     < Find crimes in grid: 0.18951099999998178 seconds>\n",
      "     < Find crimes w.in 1mi: 1.1340139999999792 seconds>\n",
      "     < Add year counts: 0.20091299999995726 seconds>:\n",
      ">>> Processed 307500013721 in 1.5259260000000268 seconds.\n",
      "     < Find crimes in grid: 0.2719390000000317 seconds>\n",
      "     < Find crimes w.in 1mi: 2.932876999999962 seconds>\n",
      "     < Add year counts: 0.35618900000002895 seconds>:\n",
      ">>> Processed 307500013753 in 3.562460999999985 seconds.\n",
      "     < Find crimes in grid: 0.17532099999999673 seconds>\n",
      "     < Find crimes w.in 1mi: 0.705742999999984 seconds>\n",
      "     < Add year counts: 0.17701399999998557 seconds>:\n",
      ">>> Processed 307500013771 in 1.0596459999999865 seconds.\n",
      "     < Find crimes in grid: 0.1768690000000106 seconds>\n",
      "     < Find crimes w.in 1mi: 0.7947110000000066 seconds>\n",
      "     < Add year counts: 0.1587009999999509 seconds>:\n",
      ">>> Processed 307500013811 in 1.131659999999954 seconds.\n",
      "     < Find crimes in grid: 0.16867400000000998 seconds>\n",
      "     < Find crimes w.in 1mi: 0.47642999999999347 seconds>\n",
      "     < Add year counts: 0.1209479999999985 seconds>:\n",
      ">>> Processed 307500014004 in 0.7674590000000308 seconds.\n",
      "     < Find crimes in grid: 0.21534299999996165 seconds>\n",
      "     < Find crimes w.in 1mi: 1.866316999999981 seconds>\n",
      "     < Add year counts: 0.1484970000000203 seconds>:\n",
      ">>> Processed 307500014009 in 2.2314790000000357 seconds.\n",
      "     < Find crimes in grid: 0.1620149999999967 seconds>\n",
      "     < Find crimes w.in 1mi: 0.31992099999996526 seconds>\n",
      "     < Add year counts: 0.10991500000000087 seconds>:\n",
      ">>> Processed 307500014023 in 0.5933630000000107 seconds.\n",
      "     < Find crimes in grid: 0.21127000000001317 seconds>\n",
      "     < Find crimes w.in 1mi: 1.9896400000000085 seconds>\n",
      "     < Add year counts: 0.28909400000003416 seconds>:\n",
      ">>> Processed 307500014075 in 2.491406999999981 seconds.\n",
      "     < Find crimes in grid: 0.18209400000000642 seconds>\n",
      "     < Find crimes w.in 1mi: 0.6787690000000453 seconds>\n",
      "     < Add year counts: 0.12026300000002266 seconds>:\n",
      ">>> Processed 307500014177 in 0.9825779999999895 seconds.\n",
      "     < Find crimes in grid: 0.15693799999996827 seconds>\n",
      "     < Find crimes w.in 1mi: 0.31577900000002046 seconds>\n",
      "     < Add year counts: 0.10591900000002852 seconds>:\n",
      ">>> Processed 307500014224 in 0.5801240000000121 seconds.\n",
      "     < Find crimes in grid: 0.18830500000001393 seconds>\n",
      "     < Find crimes w.in 1mi: 1.3393479999999727 seconds>\n",
      "     < Add year counts: 0.13334300000002486 seconds>:\n",
      ">>> Processed 307500014233 in 1.66237000000001 seconds.\n",
      "     < Find crimes in grid: 0.20091800000000148 seconds>\n",
      "     < Find crimes w.in 1mi: 1.3353970000000004 seconds>\n",
      "     < Add year counts: 0.16311899999999468 seconds>:\n",
      ">>> Processed 307500014255 in 1.700883000000033 seconds.\n",
      "     < Find crimes in grid: 0.1321360000000027 seconds>\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Must pass DataFrame with boolean values only",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-775114f4af63>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# run function on a singel school(got rid of print statments first.)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mgridbased_crimecount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschool_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfelony_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# BETTER, but time is still a struggle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# ... seems to take about 5 seconds per school (once the felony data is asigned grids.)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-35-921137226559>\u001b[0m in \u001b[0;36mgridbased_crimecount\u001b[0;34m(school_df, felony_df)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mtick3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# for debugging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mlocal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfelony_loc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mis_in_radius\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0mlocal_crimes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfelony_loc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlocal\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0mtock3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# for debugging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'     < Find crimes w.in 1mi: {} seconds>'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtock3\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtick3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# for debugging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python3.5/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1963\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1964\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1965\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1966\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mis_mi_columns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1967\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python3.5/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_getitem_frame\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2035\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2036\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbool_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2037\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Must pass DataFrame with boolean values only'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2038\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2039\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Must pass DataFrame with boolean values only"
     ]
    }
   ],
   "source": [
    "# run function\n",
    "gridbased_crimecount(school_df, felony_df)\n",
    "\n",
    "# BETTER, but time is still a struggle\n",
    "# ... seems to take about 5 seconds per school (once the felony data is asigned grids.)\n",
    "# ... in the first pass below I got through about 50 schools but then ran in to an error.\n",
    "# ... the collated data from those 50 schools is saved to: 'Collated_data_for_50schools.csv'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
