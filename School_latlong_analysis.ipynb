{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lat Long Analysis\n",
    "\n",
    "This portion of our project uses the schools' latitude and longitude (previously obtained using geocoder) to perform some final cleaning on the VADIR data and then join it to the NYC crime data by location. The code below will:  \n",
    "\n",
    "* __Load vadir data__ and cleaning it with the functions from cleandata.py  \n",
    "* Ensure __schools are consistently named__ (we'll use the names from the lat long file and join them using the beds/seds code. This also means that we'll discard records from the schools for which we don't have lat/long)  \n",
    "* __Fill in missing boroughs__ for records from 2006-2007.  \n",
    "* __Identify felonies within a 1 mile__ radius of a given school.  \n",
    "* __Plot correlations__ between school indicents and felonies (by year, by borough, by felony type, by location, by school incident type)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "% matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from vincenty import vincenty\n",
    "import cleandata as cd\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in and clean the VADIR data from the 2006-2014 school years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... data from VADIR_2006.xls appended. Added 1455 rows for a total of 1455.\n",
      "... data from VADIR_2007.xls appended. Added 1500 rows for a total of 2955.\n",
      "... data from VADIR_2008.xls appended. Added 1545 rows for a total of 4500.\n",
      "... data from VADIR_2009.xls appended. Added 1531 rows for a total of 6031.\n",
      "... data from VADIR_2010.xls appended. Added 1678 rows for a total of 7709.\n",
      "... data from VADIR_2011.xls appended. Added 1693 rows for a total of 9402.\n",
      "... data from VADIR_2012.xls appended. Added 1735 rows for a total of 11137.\n",
      "... data from VADIR_2013.xls appended. Added 1792 rows for a total of 12929.\n",
      "... data from VADIR_2014.xls appended. Added 1805 rows for a total of 14734.\n"
     ]
    }
   ],
   "source": [
    "# Raw data for each year\n",
    "RAW_DATA_DICT = {2006: 'VADIR_2006.xls', 2007: 'VADIR_2007.xls', 2008: 'VADIR_2008.xls', \n",
    "                 2009: 'VADIR_2009.xls', 2010: 'VADIR_2010.xls', 2011: 'VADIR_2011.xls', \n",
    "                 2012: 'VADIR_2012.xls', 2013: 'VADIR_2013.xls', 2014: 'VADIR_2014.xls'}\n",
    "\n",
    "# Duplicate name columns in raw files (and their replacements)\n",
    "DUP_COLS = {'County Name':'County', 'District Name': 'District', 'BEDS CODE': 'BEDS Code', \n",
    "            'False Alarm':'Bomb Threat False Alarm',\n",
    "            'Other Sex offenses': 'Other Sex Offenses', \n",
    "            'Use Possession or Sale of Drugs': 'Drug Possession', \n",
    "            'Use Possession or Sale of Alcohol': 'Alcohol Possession',\n",
    "            'Other Disruptive Incidents': 'Other Disruptive Incidents', \n",
    "            'Drug Possesion': 'Drug Possession', 'Alcohol Possesion': 'Alcohol Possession', \n",
    "            'Other Disruptive': 'Other Disruptive Incidents'}\n",
    "\n",
    "# Read in raw data and correct duplicate columns\n",
    "vadir_df = cd.vadir_concat_dfs(RAW_DATA_DICT, DUP_COLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reorder columns putting demographic information first.\n",
    "DEMO_COLS = ['School Name', 'School Type', 'School Year', 'BEDS Code',  'County', \n",
    "             'District', 'Enrollment', 'Grade Organization', 'Need/Resource Category']\n",
    "vadir_df = cd.vadir_reorder_columns(vadir_df, DEMO_COLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create Columns for \"Total incidents\", \"Incidents w/out Weapons\" and \"Incidents w/ Weapons\"\n",
    "COLUMNS = vadir_df.columns.tolist()\n",
    "INCIDENT_COLS = [c for c in COLUMNS if c not in DEMO_COLS]\n",
    "vadir_df = cd.vadir_create_tallies(vadir_df, INCIDENT_COLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cleaning VADIR data...\n",
    "# ... consistently name county and school type, fix name capitalization, remove comment rows.\n",
    "school_df = cd.vadir_clean_concat_df(vadir_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Take a look\n",
    "school_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Join School Data and Location Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read in lat-long file that contains correct names and locations\n",
    "#... ??? replace this call with function call to geocoder ???\n",
    "latlon_df = pd.read_csv('SchoolLatLon.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# take a look\n",
    "#latlon_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ensure BEDS and SED are integers so that they'll be recognized as identical\n",
    "latlon_df[\"SED CODE\"] = latlon_df[\"SED CODE\"].astype(np.int64)\n",
    "school_df[\"BEDS Code\"] = school_df[\"BEDS Code\"].astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# join latlong data to school data using the BEDS code\n",
    "school_df = pd.merge(school_df, latlon_df, left_on=['BEDS Code'],right_on=['SED CODE'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# drop the now redundant SED code\n",
    "school_df.drop(['SED CODE'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... 1967 unique schools,\n",
      "... of which 1807 have lat/long\n",
      "... and 160 are missing lat/long\n"
     ]
    }
   ],
   "source": [
    "# Take a look at the resulting data/missing values\n",
    "print('... {} unique schools,'.format(len(school_df['BEDS Code'].unique())))\n",
    "schools_withloc = school_df[school_df['latlon'].notnull()]['BEDS Code'].unique()\n",
    "schools_missingloc = school_df[school_df['latlon'].isnull()]['BEDS Code'].unique()\n",
    "print('... of which {} have lat/long'.format(len(schools_withloc)))\n",
    "print('... and {} are missing lat/long'.format(len(schools_missingloc)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fix School Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... original dataset has 3135 unique school names\n",
      "... but only 1967 unique BEDS Codes\n"
     ]
    }
   ],
   "source": [
    "# The problem:\n",
    "print('... original dataset has {} unique school names'.format(len(school_df['School Name'].unique())))\n",
    "print('... but only {} unique BEDS Codes'.format(len(school_df['BEDS Code'].unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Helper Function\n",
    "def fix_case(x):\n",
    "    \"\"\"Function to put a school name in the correct case\"\"\"\n",
    "    if not x:\n",
    "        return x\n",
    "    elif x[:3] in ['PS ', 'JHS', 'MS ']:\n",
    "        return x[:3] + x[3:].title()\n",
    "    else:\n",
    "        return x.title()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fix missing LEGAL NAMES with School Name\n",
    "school_df['LEGAL NAME'].fillna(school_df['School Name'], inplace=True)\n",
    "\n",
    "# Fix case and reassign to School Name\n",
    "school_df['School Name'] = school_df['LEGAL NAME'].apply(fix_case)\n",
    "\n",
    "# drop the now redundant LEGAL NAME column\n",
    "school_df.drop(['LEGAL NAME'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... new dataset has 1959 unique school names.\n",
      "... and 1967 unique BEDS Codes.\n"
     ]
    }
   ],
   "source": [
    "# Problem Solved\n",
    "print('... new dataset has {} unique school names.'.format(len(school_df['School Name'].unique())))\n",
    "print('... and {} unique BEDS Codes.'.format(len(school_df['BEDS Code'].unique())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill in missing boroughs\n",
    "\n",
    "__TODO:__ From the numbers below it looks like some of the County values got switched around (eg. decrease in Manhattan counts?)... I think we probalby need a way of creating the county_map dictionary that prioritizes the traditional borough name). Come back to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... 3076 entries are missing county info\n",
      "Other county tallies:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Brooklyn              3566\n",
       "Bronx                 2737\n",
       "Queens                2260\n",
       "Manhattan             1228\n",
       "New York              1040\n",
       "Staten Island          478\n",
       "Nyc Central Office     344\n",
       "Nassau                   1\n",
       "Name: County, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The first problem:\n",
    "print('... {} entries are missing county info'.format(sum(school_df['County'].isnull())))\n",
    "print('Other county tallies:')\n",
    "school_df.County.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create dictionary of county by BEDS Code\n",
    "c = school_df[school_df['County'].notnull()][['BEDS Code','County']].to_dict()\n",
    "county_map = {c['BEDS Code'][idx]: c['County'][idx] for idx in c['County'].keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# map counties using dictionary\n",
    "school_df.County = school_df['BEDS Code'].map(county_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Now only 3 entries are missing county info\n",
      "Other county tallies:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Brooklyn              4637\n",
       "Bronx                 3536\n",
       "Queens                2961\n",
       "New York              2856\n",
       "Staten Island          639\n",
       "Manhattan               80\n",
       "Nassau                   9\n",
       "Nyc Central Office       9\n",
       "Name: County, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The first problem Solved\n",
    "print('... Now only {} entries are missing county info'.format(sum(school_df['County'].isnull())))\n",
    "print('Other county tallies:')\n",
    "school_df.County.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    1967\n",
       "Name: County, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Are any BEDS Codes are linked with more than one Borough(County)?\n",
    "school_df.groupby('BEDS Code')['County'].apply(lambda x: len(x.unique())).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prep for distance sorting crime locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helper function -- check dist\n",
    "def is_in_radius(school_point, crime_point, radius):\n",
    "    \"\"\"\n",
    "    Function using vincenty package to check distance between school and crime.\n",
    "    INPUT: (lat,long) tuples for school and crime (in degrees), radius in miles.\n",
    "    OUTPUT: Boolean\n",
    "    \"\"\"\n",
    "    return vincenty(school_point, crime_point, miles=True) <= radius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helper function -- extract lat/long from object type\n",
    "def parse_latlong(dataframe, loc_column):\n",
    "    \"\"\"\n",
    "    Function to extract lat/long coords. \n",
    "    INPUT: dataframe and name of column with string tuple or list pair of coordinates.\n",
    "    OUTPUT: n/a. Function modifies dataframe to add a lat and long column with float type.\n",
    "    \"\"\"\n",
    "    get_lat = lambda x: x.split(',')[0][1:] if type(x)==type('s') else np.nan\n",
    "    get_long = lambda x: x.split(',')[1][:-1] if type(x)==type('s') else np.nan\n",
    "    dataframe['lat'] = dataframe[loc_column].apply(get_lat).astype('float64')\n",
    "    dataframe['long'] = dataframe[loc_column].apply(get_long).astype('float64')\n",
    "    print('... latitude and longitude extracted for dataframe.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load NYC dataframe\n",
    "felony_df = pd.read_csv('NYPD_7_Major_Felony_Incidents.csv', index_col = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ERROR?: the cleaning function seems to take forever... need to take a closer look/maybe fix it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ... and clean it\n",
    "#felony_df = cd.clean_NYPD(felony_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... latitude and longitude extracted for dataframe.\n",
      "... latitude and longitude extracted for dataframe.\n"
     ]
    }
   ],
   "source": [
    "# Extact Lattitude and longitude data for both dataframes\n",
    "parse_latlong(school_df, 'latlon')\n",
    "parse_latlong(felony_df, 'Location 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance:  13.120085\n",
      "... w/in 2 mi? False\n",
      "... w/in 50 mi? True\n"
     ]
    }
   ],
   "source": [
    "# Testing vincenty on the first felony and first school\n",
    "first_school_point = (school_df.loc[0,'lat'], school_df.loc[0,'long']) \n",
    "first_felony_point = (felony_df.loc[1,'lat'], felony_df.loc[1,'long']) \n",
    "\n",
    "# not w/in 2 miles, but yes, w/in 50\n",
    "print('Distance: ', vincenty(first_school_point, first_felony_point))\n",
    "print(\"... w/in 2 mi?\", is_in_radius(first_school_point, first_felony_point, 2))\n",
    "print(\"... w/in 50 mi?\",is_in_radius(first_school_point, first_felony_point, 50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to extract crime tallies w/in radius of schools\n",
    "\n",
    "I suspect that looping through each school is going to take forever... but we'll try that first and then explore a smarter (dynamic programming) alternative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... there are 640 missing latlon entries\n",
      "... there are 640 missing lat entries\n"
     ]
    }
   ],
   "source": [
    "# Quick Check, are there rows with 'latlon' but not 'lat'\n",
    "print('... there are {} missing latlon entries'.format(sum(school_df.latlon.isnull())))\n",
    "print('... there are {} missing lat entries'.format(sum(school_df.lat.isnull())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 1: Helper functions for setting up a grid for NYC lat/long coords\n",
    "NOTES: The max lat of a school is ~ 40.9  and the distance between 40.9 and 40.95 is over 3 miles... but there are 7 crimes that fell under the jurisdiction of the NY Transit police whose locations are recorded north of 41 degrees (the farthes one is 500 miles away). The minimum longitude of a school is ~-74.24 which is around 3 miles from -74.3. There are 63 crimes that occurred west of -74.3. I suggest that we disregard these outliers for the purposes of our analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lat ranges from 40.5078027 to 40.95 with a total distance of 30.512686\n",
      "Long ranges from -74.3 to -73.4883134 with a total distance of 56.290003\n"
     ]
    }
   ],
   "source": [
    "# Initial exploration of ranges\n",
    "max_lat = 40.95\n",
    "min_lat = min(felony_df.lat.max(), school_df.lat.min())\n",
    "max_long = max(felony_df.long.max(), school_df.long.max())\n",
    "min_long = -74.3\n",
    "\n",
    "lat_dist = vincenty((min_lat, 0.5*(max_long + min_long)),(max_lat, 0.5*(max_long + min_long)), miles=True)\n",
    "long_dist = vincenty((min_long, 0.5*(max_lat + min_lat)),(max_long, 0.5*(max_lat + min_lat)), miles=True)\n",
    "\n",
    "print('Latitude ranges from {} to {} with a total distance of {}'.format(min_lat, max_lat, lat_dist))\n",
    "print('Longitude ranges from {} to {} with a total distance of {}'.format(min_long, max_long, long_dist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Helper function to identify grid cell that contains a given point\n",
    "def nyc_grid(lat,long):\n",
    "    \"\"\"\n",
    "    This function identifies a square mile cell of NYC that contains \n",
    "    the given longitude and latitude point. There are 1500 cells in \n",
    "    total. 30 rows each represent a segement of latitude and 50 \n",
    "    columns each represent a segment of longitude. The cells are \n",
    "    numbered 0 through 1599 and they are unique to this analysis.\n",
    "    \"\"\"\n",
    "    # max and min values from data set\n",
    "    max_lat = 40.95\n",
    "    min_lat = 40.50\n",
    "    max_long = -73.45\n",
    "    min_long = -74.30\n",
    "    \n",
    "    # divide each range into segments of a little over a mile\n",
    "    delta_lat = (max_lat - min_lat)/28\n",
    "    delta_long = (max_long - min_long)/48\n",
    "\n",
    "    # then segment each direction\n",
    "    lat_seg = np.array([min_lat + idx*delta_lat for idx in range(-1,29)])\n",
    "    long_seg = np.array([min_long + idx*delta_long for idx in range(-1,49)])\n",
    "\n",
    "    # identify where given point fits in segments\n",
    "    row = sum(lat_seg <= lat) - 1\n",
    "    col = sum(long_seg <= long) - 1\n",
    "    \n",
    "    # return grid number\n",
    "    if row < 0 or row == 29 or col < 0 or col == 49:\n",
    "        return np.nan\n",
    "    else:\n",
    "        return row * 50 + col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test an out of bound point\n",
    "nyc_grid(40.653161, -76.862164)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1074"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test an in bound point\n",
    "nyc_grid(40.821798, -73.886463)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# helper function to get a list of adjacent cells\n",
    "def get_adjacent(cell_num):\n",
    "    \"\"\" \n",
    "    This function identifies a group of cells which together superset \n",
    "    any points within a mile of any location in the original cell.\n",
    "    INPUT: a cell number (< 5999) from NYC grid\n",
    "    OUTPUT: a list of adjacent and or diagonal cell numbers\n",
    "    \n",
    "    NOTE: this function should only be run on cell numbers of vadir\n",
    "    school locations since the nyc_grid is designed so that all\n",
    "    schools are in a cell that is not a boarder cell.\n",
    "    \"\"\"\n",
    "    col = cell_num % 50\n",
    "    row = cell_num // 50\n",
    "    row_range = [row - 1, row, row + 1]\n",
    "    col_range = [col - 1, col, col + 1]\n",
    "    return [r * 50 + c for r in row_range for c in col_range]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 51, 52, 53, 101, 102, 103]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test grid adjacency\n",
    "get_adjacent(52)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 2: Crime counting function to search only within adjacent cells of the school\n",
    "\n",
    "NOTE: loading grid cell#s for the felony data set takes 3-4 minutes and only needs to be done once. I've included it (commented out) in the function... but you should uncomment it in the final version of our notebook as well as the first time you load the felony data each day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gridbased_crimecount(school_df, felony_df):\n",
    "    \"\"\"\n",
    "    Grid based function to identify felonies w/in one mile of each school.\n",
    "    INPUT: school df w/ cols 'latlon', 'lat', 'long', and 'School Year'\n",
    "           felony df w/ cols 'Occurrence Year', 'lat','long','Offense', and 'Identifier'\n",
    "    OUTPUT: n/a, modifies school data.\n",
    "    \"\"\"\n",
    "    # prepare felony dataframe by adding a column for nyc_grid cell number\n",
    "#     tick0 = time.clock() # for debugging\n",
    "#     felony_df.lat.fillna(0, inplace=True)\n",
    "#     felony_df.long.fillna(0, inplace=True)\n",
    "#     felony_df['NYC_grid'] = felony_df.apply(lambda x: nyc_grid(x.lat, x.long),axis=1)\n",
    "#     tock0 = time.clock() # for debugging\n",
    "#     print('< It took {} seconds to calculate all of the felony grids.'.format(tock0 - tick0))\n",
    "    \n",
    "    # Initialize new columns in school data frame\n",
    "    school_df['CrimeIDS'] = pd.Series()\n",
    "    school_df['Total Felonies w/in 1mi'] = pd.Series()\n",
    "    school_df['Grand Larceny w/in 1mi'] = pd.Series()\n",
    "    school_df['Robbery w/in 1mi'] = pd.Series()\n",
    "    school_df['Burglary w/in 1mi'] = pd.Series()\n",
    "    school_df['Assault w/in 1mi'] = pd.Series()\n",
    "    school_df['Auto Theft w/in 1mi'] = pd.Series()\n",
    "    school_df['Rape w/in 1mi'] = pd.Series()\n",
    "    school_df['Murders w/in 1mi'] = pd.Series()\n",
    "    \n",
    "    # Group schools by location (school BEDS code) \n",
    "    grouped = school_df[school_df.lat.notnull()].groupby(['BEDS Code'])\n",
    "    print('... found {} unique schools'.format(len(grouped.groups))) # for debugging\n",
    "    \n",
    "    # Loop through schools & years\n",
    "    tick = time.clock() # for debugging\n",
    "    for beds, df in grouped:\n",
    "        tick1 = time.clock() # for debugging\n",
    "        #print('>>> now processing:', beds) # for debugging\n",
    "        # NOTE: the coordinates should all be the same so the mean is just the location\n",
    "        assert len(df.lat.unique().tolist()) == 1, 'ERROR: multiple latitudes for this school.'\n",
    "        location = (df.lat.mean(), df.long.mean())\n",
    "        \n",
    "        #tick2 = time.clock() # for debugging\n",
    "        cells_to_search = get_adjacent(nyc_grid(*location)) \n",
    "        felony_loc = felony_df.loc[felony_df.NYC_grid.isin(cells_to_search)]\n",
    "        ###felony_loc = felony_df[felony_df.NYC_grid.apply(lambda x: x in cells_to_search)]\n",
    "        #print('    ... found {} crimes w/in 9 cells'.format(len(felony_loc))) # for debugging\n",
    "        local = felony_loc.apply(lambda x: is_in_radius(location, (x.lat, x.long), 1), axis=1)\n",
    "        local_crimes = felony_loc[local]\n",
    "        #print('    ... of which {} are w/in one mile'.format(len(local_crimes))) # for debugging\n",
    "        #tock2 = time.clock() # for debugging\n",
    "        #print('< Find nearby crimes {} seconds>'.format(tock2 - tick2))  # for debugging     \n",
    "        \n",
    "        # tally and store felonies for each year\n",
    "        for year in df['School Year'].unique():\n",
    "            #tick3 = time.clock()  # for debugging\n",
    "            # school_df indices for this year\n",
    "            idxs = df[df['School Year'] == year].index.tolist()\n",
    "            # subset of crimes w/in 1 mi that occurred this year\n",
    "            subset = local_crimes[local_crimes['Occurrence Year'] == year]\n",
    "            #print('    ... of which {} occurred in '.format(len(subset), year))  # for debugging \n",
    "            \n",
    "            # tally and store felony counts\n",
    "            school_df.loc[idxs,['CrimeIDS']] = str(subset.Identifier.unique().tolist())\n",
    "            school_df.loc[idxs,['Total Felonies w/in 1mi']] = len(subset)        \n",
    "            school_df.loc[idxs,['Grand Larceny w/in 1mi']] = sum(subset['Offense'] == 'GRAND LARCENY')\n",
    "            school_df.loc[idxs,['Robbery w/in 1mi']] = sum(subset['Offense'] == 'ROBBERY')\n",
    "            school_df.loc[idxs,['Burglary w/in 1mi']] = sum(subset['Offense'] == 'BURGLARY')\n",
    "            school_df.loc[idxs,['Assault w/in 1mi']] = sum(subset['Offense'] == 'FELONY ASSAULT')\n",
    "            school_df.loc[idxs,['Auto Theft w/in 1mi']] = sum(subset['Offense'] == 'GRAND LARCENY OF MOTOR VEHICLE')\n",
    "            school_df.loc[idxs,['Rape w/in 1mi']] = sum(subset['Offense'] == 'RAPE')\n",
    "            school_df.loc[idxs,['Murders w/in 1mi']] = sum(subset['Offense'] == 'MURDER & NON-NEGL. MANSLAUGHTE')\n",
    "            #tock3 = time.clock()\n",
    "            #print('< Add counts for the year: {} seconds>:'.format(tock3 - tick3))   # for debugging  \n",
    "        tock1 = time.clock() # for debugging  \n",
    "        print('>>> Processed {} in {} seconds.'.format( beds, tock1 - tick1)) # for debugging  \n",
    "    tock = time.clock()  # for debugging  \n",
    "    print(' TOTAL TIME:', tock - tick) # for debugging  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "//anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "//anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "//anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "//anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "//anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "//anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "//anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "//anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... found 4 unique schools\n",
      ">>> now processing: 320700860852\n",
      "< Tally felonies for one school: 5.161850000000015 seconds>\n",
      ">>> now processing: 320800860846\n",
      "< Tally felonies for one school: 4.477165999999954 seconds>\n",
      ">>> now processing: 321100860855\n",
      "< Tally felonies for one school: 2.3801900000000273 seconds>\n",
      ">>> now processing: 321100860859\n",
      "< Tally felonies for one school: 3.5629490000000033 seconds>\n",
      " TOTAL TIME: 15.586018999999993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python3.5/site-packages/pandas/core/indexing.py:426: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n"
     ]
    }
   ],
   "source": [
    "# run function (got rid of print statments first.)\n",
    "gridbased_crimecount(school_subset, felony_df)\n",
    "\n",
    "# BETTER, but time is still a struggle\n",
    "# ... seems to take about 5 seconds per school (once the felony data is asigned grids.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
